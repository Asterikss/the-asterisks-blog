{
  "hash": "3c283536faecf2e8e5fbfb43b7098462",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Different ways of counting PyTorch model FLOPs (library compilation)\"\nauthor: \"Andre Mirończuk\"\ndate: \"2025-01-27\"\ncategories: [pytorch, FLOPs]\nimage: \"./how_to_count_flops_tile.png\"\n---\n\n\nProfiling libraries in one spot.\n\nKeep in mind that automatic FLOP measurements are approximations and can be\nimprecise, particularly when faced with non-standard layers. Custom kernels, for\nexample, will be outright skipped if no formulas or specific values are manually\nregistered for them. Similarly, unsupported operations will not contribute to the\nfinal estimation; sparse tensors may yield the same FLOP counts as their dense\ncounterparts. And so on...\n\nThat being said, those numbers are still accurate enough in a lot of scenarios and\ncan be quite handy, especially when comparing similar architectures (uumh actually,\nalgorithms running on those architectures).\n\nYou can check your intuition by asking yourself: What is the FLOP count for an\nembedding layer? Backward pass? What about fine-tuning a model with the embedding\nlayer being frozen? A de-embedding layer?\n\n::: {#1e60a749 .cell execution_count=1}\n``` {.python .cell-code}\nfrom torchvision.models import wide_resnet50_2\nimport torch\n\nwide_resnet = wide_resnet50_2(weights=None)\ninput_shape = (1, 3, 244, 244)\ninput_tensor = torch.randn(input_shape)\n```\n:::\n\n\n### Option 1: PyTorch's inbuilt FLOP counter\n\nAs of now this module is actually undocumented. More info in the resources part. This\ncode takes into consideration both forward and backward pass.\n\n::: {#fc184414 .cell execution_count=2}\n``` {.python .cell-code}\nfrom torch.utils.flop_counter import FlopCounterMode\n\nflop_counter = FlopCounterMode(display=False, depth=None)\nwith flop_counter:\n  wide_resnet(input_tensor).sum().backward()\ntotal_flops_one_fwd_bwd: int = flop_counter.get_total_flops()\nprint(f\"Total GigaFLOPs one forward-backward: {total_flops_one_fwd_bwd / 1e9}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal GigaFLOPs one forward-backward: 86.000790528\n```\n:::\n:::\n\n\n- Resources:\n  - [dev-discuss.pytorch.org/t/the-ideal-pytorch-flop-counter-with-torch-dispatch/505](https://dev-discuss.pytorch.org/t/the-ideal-pytorch-flop-counter-with-torch-dispatch/505)\n    - [colab.research.google.com/drive/1zjAisRrc8R6uixKsrs1DRm3lwz5MWN68#scrollTo=w9Dezwu6PWtW](https://colab.research.google.com/drive/1zjAisRrc8R6uixKsrs1DRm3lwz5MWN68#scrollTo=w9Dezwu6PWtW)\n  - [github.com/pytorch/pytorch/issues/123800](https://github.com/pytorch/pytorch/issues/123800)\n  - [github.com/pytorch/pytorch/issues/5013](https://github.com/pytorch/pytorch/issues/5013)\n  - [github.com/pytorch/pytorch/blob/main/torch/utils/flop_counter.py](https://github.com/pytorch/pytorch/blob/main/torch/utils/flop_counter.py)\n\n\n### Option 2: torchinfo\n\nThis time only the forward pass.\n\n::: {#0cfea1fd .cell execution_count=3}\n``` {.python .cell-code}\nfrom torchinfo import summary\nmodel_stats = summary(wide_resnet, input_size=input_shape, verbose=0)\nmodel_stats\n```\n:::\n\n\n::: {#7a401cce .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"print model_stats reasonably\"}\nfrom IPython.display import Markdown, display\nsummary_str = str(model_stats)\n\ndisplay(Markdown(\"```\\n\" + summary_str[:500] + \"\\n```\"))\nprint(\"...\")\ndisplay(Markdown(\"```\\n\" + summary_str[-645:] + \"\\n```\"))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n```\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet                                   [1, 1000]                 --\n├─Conv2d: 1-1                            [1, 64, 122, 122]         9,408\n├─BatchNorm2d: 1-2                       [1, 64, 122, 122]         128\n├─ReLU: 1-3                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n...\n```\n:::\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n```\n├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n├─Linear: 1-10                           [1, 1000]                 2,049,000\n==========================================================================================\nTotal params: 68,883,240\nTrainable params: 68,883,240\nNon-trainable params: 0\nTotal mult-adds (G): 14.38\n==========================================================================================\nInput size (MB): 0.71\nForward/backward pass size (MB): 282.22\nParams size (MB): 275.53\nEstimated Total Size (MB): 558.47\n==========================================================================================\n```\n:::\n:::\n\n\n`Total mult-adds (G): 14.38` is of intrest to us (`Giga-MACs` = 1B (10⁹) MACs). So\n28.76 GFLOPs.\n\nGiven that it is often assumed that backward pass has about twice the amount of FLOPs as\nthe forward one, this checks out. 28.76 * 3 = 86.28, which is close to\nPyTorch's FlopCounterMode output.\n\n- Resources:\n  - [github.com/TylerYep/torchinfo](https://github.com/TylerYep/torchinfo)\n\n### Option3: deepspeed\n\n```python\nfrom deepspeed.profiling.flops_profiler import get_model_profile\nfrom deepspeed.accelerator import get_accelerator\n\n# with get_accelerator().device(0):\nflops, macs, params = get_model_profile(\n  model=wide_resnet,\n  input_shape=input_shape,\n  args=None,\n  kwargs=None,\n  print_profile=True,\n  detailed=True,\n  module_depth=-1,\n  top_modules=1,\n  warm_up=10,\n  as_string=True,\n  output_file=None,\n  ignore_modules=None,\n)\n\nprint(flops, macs, params)\nprint(\"Params:\", params)\nprint(\"GMACs:\", macs)\nprint(\"GFLOPs:\", flops)\n```\n\n```text\nParams: 68.88 M\nGMACs: 14.38 GMACs\nGFLOPs: 28.81 G\n```\n\nBesides these values, deepspeed outputs quite a detailed profiling report.\n\n- Resources:\n  - [deepspeed.ai/tutorials/flops-profiler](https://www.deepspeed.ai/tutorials/flops-profiler/)\n\n### Option4: fvcore\n\n::: {#74ad1abe .cell execution_count=5}\n``` {.python .cell-code}\nfrom fvcore.nn import FlopCountAnalysis\n\nflops = FlopCountAnalysis(wide_resnet, input_tensor)\nprint(\"FLOPs: \", flops.total())\nprint(f\"GFLOPs: {flops.total() / 1e9}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUnsupported operator aten::add_ encountered 69 time(s)\nUnsupported operator aten::max_pool2d encountered 1 time(s)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFLOPs:  14468464384\nGFLOPs: 14.468464384\n```\n:::\n:::\n\n\nWell, interestingly, we got almost exactly as many FLOPs as MACs with torchinfo and\ndeepspeed.  \nEncounter counter does not disappoint.\n\n- Resources:\n  - [github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md](https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md)\n\n### Option5: ptflops\n\n```python\nfrom ptflops import get_model_complexity_info\n\nmacs, params = get_model_complexity_info(\n  wide_resnet, input_shape[1:], as_strings=False, print_per_layer_stat=False, backend='pytorch'\n)\nprint(\"Params:\", params)\nprint(\"GMACs:\", macs / 1e9)\nprint(\"GFLOPs:\", (macs / 1e9) * 2)\n```\n\n```text\nParams: 68883240\nGMACS: 14.44918756\nGFLOPS: 28.89837512\n```\n\n- Resources:\n  - [github.com/sovrasov/flops-counter.pytorch](https://github.com/sovrasov/flops-counter.pytorch)\n\n### Option6: flopth\n\n```python\nfrom flopth import flopth\n\nflops, params = flopth(wide_resnet, in_size=input_shape[1:])\nprint(\"Params:\", params)\nprint(\"GFLOPs:\", flops)\n```\n\n```text\nParams: 68.8832M\nGFLOPs: 14.4242G\n```\n\n- Resources:\n  - [github.com/vra/flopth](https://github.com/vra/flopth)\n\n### Option7: calflops\n\n```python\nfrom calflops import calculate_flops\n\nflops, macs, params = calculate_flops(\n  model=wide_resnet, input_shape=input_shape, output_as_string=True, output_precision=4\n)\n\nprint(\"Params:\", params)\nprint(\"GMACs:\", macs[:7])\nprint(\"GFLOPs:\", flops[:7])\n```\n\n```text\nParams: 68.8832 M\nGMACs: 14.3801\nGFLOPs: 28.8124\n```\n\n- Resources:\n  - [github.com/MrYxJ/calculate-flops.pytorch?tab=readme-ov-file](https://github.com/MrYxJ/calculate-flops.pytorch?tab=readme-ov-file)\n  - Online hg frontend:\n    - [huggingface.co/spaces/MrYXJ/calculate-model-flops](https://huggingface.co/spaces/MrYXJ/calculate-model-flops)\n\n### Option8: thop\n\n```python\nfrom thop import profile\n\nmacs, params = profile(wide_resnet, inputs=(input_tensor, ))\nprint(\"Params:\", params)\nprint(\"GMACs:\", macs / 1e9)\n```\n\n```text\nParams: 68883240.0\nGMACs: 43.352484096\n```\n\n43 is quite a number when compared to other profilers.\n\n- Resources:\n  - [github.com/Lyken17/pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter)\n  - [bnikolic.co.uk/blog/python/flops/2019/10/01/pytorch-count-flops.html](https://bnikolic.co.uk/blog/python/flops/2019/10/01/pytorch-count-flops.html)\n\n\nIf you're looking for a dedicated third-party library, I would choose between\ndeepspeed, calflops (Transformers), and fvcore (CV).\n\n##### Other Resources\n\n- Some further discussions:\n  - [github.com/tensorflow/tensorflow/pull/19792#issuecomment-415607267](https://github.com/tensorflow/tensorflow/pull/19792#issuecomment-415607267)\n  - [discuss.pytorch.org/t/calculating-flops-of-a-given-pytorch-model/3711/7](https://discuss.pytorch.org/t/calculating-flops-of-a-given-pytorch-model/3711/7)\n\n- More profilers:\n  - [mmcv.readthedocs.io/en/latest/api/generated/mmcv.cnn.get_model_complexity_info.html?highlight=flops](https://mmcv.readthedocs.io/en/latest/api/generated/mmcv.cnn.get_model_complexity_info.html?highlight=flops)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}