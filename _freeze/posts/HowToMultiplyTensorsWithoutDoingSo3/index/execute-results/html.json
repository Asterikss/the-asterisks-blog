{
  "hash": "6d504a6a493e452f1177171c9c9045d8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How to multiply tensors wihout doing so? (3/3)\"\nauthor: \"Andre Mirończuk\"\ndate: \"2024-09-27\"\ncategories: [tinygrad, pytorch, tensors]\nimage: \"./tiny-no-bg.png\"\n---\n\n\n[First part](./../HowToMultiplyTensorsWithoutDoingSo1/index.qmd)  \n[Second part](./../HowToMultiplyTensorsWithoutDoingSo2/index.qmd)\n\nLet's take [tinygrad](https://github.com/tinygrad/tinygrad) as an example.\n\nHere's the high-level code for matmul:\n\n::: {#644de238 .cell execution_count=1}\n``` {.python .cell-code}\ndef dot(self, w:Tensor, acc_dtype:Optional[DTypeLike]=None) -> Tensor:\n    n1, n2 = len(self.shape), len(w.shape)\n    assert n1 != 0 and n2 != 0, f\"both arguments to matmul need to be at least 1D, but they are {n1}D and {n2}D\"\n    if (L:=self.shape[-1]) != (R:=w.shape[-min(n2, 2)]): raise AssertionError(f\"shapes {self.shape} and {w.shape} cannot be multiplied ({L} != {R})\")\n    x = self.reshape(*self.shape[0:-1], *[1]*min(n1-1, n2-1, 1), self.shape[-1])\n    w = w.reshape(*w.shape[0:-2], *[1]*min(n1-1, n2-1, 1), *w.shape[-min(n2, 2):]).transpose(-1, -min(n2, 2))\n    return (x*w).sum(-1, acc_dtype=acc_dtype).cast(least_upper_dtype(x.dtype, w.dtype) if acc_dtype is None else acc_dtype)\n\ndef matmul(self, x:Tensor, reverse=False, acc_dtype:Optional[DTypeLike]=None) -> Tensor:\n    return x.dot(self, acc_dtype=acc_dtype) if reverse else self.dot(x, acc_dtype=acc_dtype)\n```\n:::\n\n\nIt might look intimidating, but it's actually really elegant and beautiful.  \nYou'll see.\n\nFirst lines are simple.\n\nWe will use these tensors as examples (PyTorch has very similar api, so we will\nuse it for simplicity):\n\n::: {#24fce11b .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nself = torch.rand(3, 3, 4)\nw = torch.rand(2, 1, 4, 5)\nself.shape, w.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(torch.Size([3, 3, 4]), torch.Size([2, 1, 4, 5]))\n```\n:::\n:::\n\n\nWe extract the rank of both tensors.\n\n::: {#eb29b233 .cell execution_count=3}\n``` {.python .cell-code}\nn1, n2 = len(self.shape), len(w.shape)\nn1, n2\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(3, 4)\n```\n:::\n:::\n\n\nNow we do some checks to ensure the operation we want to perform is valid:\n\n::: {#3b394a15 .cell execution_count=4}\n``` {.python .cell-code}\nassert n1 != 0 and n2 != 0, f\"both arguments to matmul need to be at least 1D, but they are {n1}D and {n2}D\"\nif (L:=self.shape[-1]) != (R:=w.shape[-min(n2, 2)]): raise AssertionError(f\"shapes {self.shape} and {w.shape} cannot be multiplied ({L} != {R})\")\n```\n:::\n\n\nThe first line is pretty self-explanatory.\n\nThe second one will establish if you can indeed matmul those tensors.\n\n`-min(n2, 2)` will evaluate to either `-1` or `-2`. Remember, `n2` cannot be 0 at\nthis point. It will compare the last dimension of `self` with the second to\nlast dimension of `w`, unless `w` is a vector (1-dimensional tensor, e.g., size =\n(3,), but not a `row vector` (e.g., size = (1, 3))). Then, since there is only one\ndimension, we will extract the last one.\n\nWill that take care of all possible shape permutations between tensors?  \nNot quite. It will handle the most important part and turn a blind eye\nto all batch dimensions. We'll touch on that later.\n\nFor two 2-by-2 tensors, it's easy to tell if you can multiply them.\n\nWhat about those (shapes)?:  \n(2, 1, 6, 4) and  \n(4, 3, 1, 5, 4, 7)\n\nCan you?\n\nAbsolutely.\n\n::: {#03afafe4 .cell execution_count=5}\n``` {.python .cell-code}\ntmp1 = torch.rand(2, 1, 6, 4)\ntmp2 = torch.rand(4, 3, 1, 5, 4, 7)\n(tmp1@tmp2).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ntorch.Size([4, 3, 2, 5, 6, 7])\n```\n:::\n:::\n\n\nWith all batch dimensions, you only compare them to their counterparts. I wrote\nabout broadcasting rules in the first part.\n\nWhat you sort of end up \"matmuling\" are the two last dimensions of both tensors:  \n(6, 4) and  \n(4, 7)\n\nOut of it comes (6, 7). The `4`s disappear, and you get the shape (4, 3, 2, 5, 6, 7).\n\nSecond line of this snippet checked if those `4`s were the same.\n\nGreat! Wait, but what about mismatching batch dimensions?\n\nThese tensors (shapes) cannot be multiplied:  \n(2, 6, 4) and  \n(3, 4, 7)\n\n::: {.reference-code}\n\n::: {#204476ed .cell execution_count=6}\n``` {.python .cell-code}\ntry:\n    torch.rand(2, 6, 4) @ torch.rand(3, 4, 7)\nexcept Exception as e:\n    print(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n```\n:::\n:::\n\n\n:::\n\nIn this situation, this error will be thrown in the first part of the return\nexpression ➫ (x*y). Namely, broadcasting will fail.\n\nNext, we will reshape our tensors just like before.\n\n::: {#f6cf11e3 .cell execution_count=7}\n``` {.python .cell-code}\nx = self.reshape(*self.shape[0:-1], *[1]*min(n1-1, n2-1, 1), self.shape[-1])\n```\n:::\n\n\nBoth tensors will be reshaped so that they become a cube after being `*`ed.\n\nThis operation will put the correct number of singleton dimensions in second to\nlast place.\n\nEasy stuff first:\n\n::: {#98706a29 .cell execution_count=8}\n``` {.python .cell-code}\nprint(*self.shape[0:-1])\nprint(self.shape[-1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3 3\n4\n```\n:::\n:::\n\n\nMoving on.\n\nNow, this is beautiful Python:\n\n::: {#e0c9fb1f .cell execution_count=9}\n``` {.python .cell-code}\n*[1]*min(n1-1, n2-1, 1)\n```\n:::\n\n\n`min(n1 - 1, n2 - 1, 1)` will evaluate to either 1 or 0. It just checks if\neither of the tensors is 1-dimensional (e.g., shape = (3,)). We have already\nseen this type of operation from before ➫ `-min(n2, 2)`.\n\n::: {#cda8b459 .cell execution_count=10}\n``` {.python .cell-code}\n*[1]*\n```\n:::\n\n\nWhat is this?\n\nIf `min(n1 - 1, n2 - 1, 1)` evaluates to `1`, `[1]` will not change. Then,\nusing `*`, it will be unpacked to just a `1`. So exactly the output of\n`min(n1 - 1, n2 - 1, 1)`.\n\n::: {#93476807 .cell execution_count=11}\n``` {.python .cell-code}\nprint(min(n1-1, n2-1, 1))\nprint(*[1] * min(3 - 1, 4 - 1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n1\n```\n:::\n:::\n\n\nSo:\n\n::: {.reference-code}\n\n::: {#b11767c1 .cell execution_count=12}\n``` {.python .cell-code}\nself.reshape(3, 3, 1, 4)\n```\n:::\n\n\n:::\n\nHowever if `min(n1 - 1, n2 - 1, 1)` evaluates to 0, then we don't want to put\nanything there (`x = self.reshape(3, 3, 4)` and not `x = self.reshape(3, 3, 0,\n4)` or `x = self.reshape(3, 3, —, 4)`).\n\nTo achieve that, we first multiply the result with `[1]`. For `1`, it will not\nchange anything, as we stated already. But for `0`, instead of `[0]`, we will\nactually get just an empty list `[]`. And when you unpack an empty list, it\nwill disappear.\n\n::: {#12c6bc6b .cell execution_count=13}\n``` {.python .cell-code}\nprint(\"♘\", *[1] * min(3 - 1, 1 - 1, 1), \"♘\")\nprint(\"♘\", \"♘\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n♘ ♘\n♘ ♘\n```\n:::\n:::\n\n\nSo:\n\n::: {.reference-code}\n\n::: {#5822091e .cell execution_count=14}\n``` {.python .cell-code}\nself.reshape(3, 3, 4)\n```\n:::\n\n\n:::\n\nInstead of:\n\n::: {.reference-code}\n\n::: {#45a69893 .cell execution_count=15}\n``` {.python .cell-code}\nself.reshape(3, 3, 0, 4)\n```\n:::\n\n\n:::\n\nGenius.\n\nLet's peek at the shape.\n\n::: {#1070d4c4 .cell execution_count=16}\n``` {.python .cell-code}\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntorch.Size([3, 3, 1, 4])\n```\n:::\n:::\n\n\nJust as we discussed.\n\nWe'll do something similar for the second tensor:\n\n::: {#144f1807 .cell execution_count=17}\n``` {.python .cell-code}\nw = w.reshape(\n    *w.shape[0:-2],\n    *[1] * min(n1 - 1, n2 - 1, 1),\n    *w.shape[-min(n2, 2):]\n).transpose(-1, -min(n2, 2))\n```\n:::\n\n\n::: {#6d558d62 .cell execution_count=18}\n``` {.python .cell-code}\nprint(*w.shape[0:-2], end=\"   \")\nprint(*[1]*min(n1 - 1, n2 - 1, 1), end=\"   \")\nprint(*w.shape[-min(n2, 2):])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2 1   1   4 5\n```\n:::\n:::\n\n\n`*w.shape[0:-2]` extracts all dimensions not including the last two (could be []).  \n`*[1]*min(n1-1, n2-1, 1)` does the same thing as before.  \n`*w.shape[-min(n2, 2):]` extracts all the left-out dimensions (either the last two or the last one).\n\nSo:\n\n::: {.reference-code}\n\n::: {#fa11ab84 .cell execution_count=19}\n``` {.python .cell-code}\nw.reshape(2, 1, 1, 4, 5)\n```\n:::\n\n\n:::\n\n::: {#c669b1cd .cell execution_count=20}\n``` {.python .cell-code}\nw = w.reshape(*w.shape[0:-2], *[1]*min(n1-1, n2-1, 1), *w.shape[-min(n2, 2):])\n```\n:::\n\n\nLet's peak at the shape (before transposing).\n\n::: {#8410cf30 .cell execution_count=21}\n``` {.python .cell-code}\nw.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\ntorch.Size([2, 1, 1, 4, 5])\n```\n:::\n:::\n\n\n<!-- TODO -->\nWe'll transpose it for the same reasons we stated in previous parts.  \nAgain `-min(n2, 2)` will check if we're working with 1-dimensional tensors.\n\n`.transpose(-1, -2)` switches the two last dimensions.  \n`.transpose(-1, -1)` does nothing.\n\nExamples:\n\n::: {.reference-code}\n\n::: {#3a450177 .cell execution_count=22}\n``` {.python .cell-code}\ntorch.rand(2, 3, 4).transpose(-1, -2).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\ntorch.Size([2, 4, 3])\n```\n:::\n:::\n\n\n:::\n\n::: {.reference-code}\n\n::: {#dad54416 .cell execution_count=23}\n``` {.python .cell-code}\ntorch.rand(2, 3, 4).transpose(-1, -1).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\ntorch.Size([2, 3, 4])\n```\n:::\n:::\n\n\n:::\n\n::: {.reference-code}\n\n::: {#a440bee8 .cell execution_count=24}\n``` {.python .cell-code}\ntry:\n    torch.rand(3).transpose(-1, -2).size\nexcept Exception as e:\n    print(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDimension out of range (expected to be in range of [-1, 0], but got -2)\n```\n:::\n:::\n\n\n:::\n\nLet's finish transposing.\n\n::: {#0040c992 .cell execution_count=25}\n``` {.python .cell-code}\nw = w.transpose(-1, -min(n2, 2))\n```\n:::\n\n\n::: {#64003b4b .cell execution_count=26}\n``` {.python .cell-code}\nw.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ntorch.Size([2, 1, 1, 5, 4])\n```\n:::\n:::\n\n\nLastly we'll multiply them and sum over the last dimension, skipping custom\nlogic regarding library's dtypes.\n\n::: {#1d2f08b1 .cell execution_count=27}\n``` {.python .cell-code}\nreturn (x * w)\n        .sum(-1, acc_dtype=acc_dtype)\n        .cast(\n            least_upper_dtype(x.dtype, w.dtype) \n            if acc_dtype is None \n            else acc_dtype\n        )\n```\n:::\n\n\n::: {#294a01ea .cell execution_count=28}\n``` {.python .cell-code}\nx.shape, w.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n(torch.Size([3, 3, 1, 4]), torch.Size([2, 1, 1, 5, 4]))\n```\n:::\n:::\n\n\nNow, broadcast into a higher dimension over the created singleton dimensions\n(for two  2-dimensional starting tensors, this would result in a cube), then\nreduce it back to the original dimension using `sum()`, which squeezes out the\nsingleton dimensions by default.\n\n::: {#cf4d005f .cell execution_count=29}\n``` {.python .cell-code}\n(x*w).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([2, 3, 3, 5, 4])\n```\n:::\n:::\n\n\n::: {#6a6ddb10 .cell execution_count=30}\n``` {.python .cell-code}\nres = (x*w).sum(-1)\n```\n:::\n\n\nAnd...\n\n::: {#3dcd7198 .cell execution_count=31}\n``` {.python .cell-code}\nres\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\ntensor([[[[0.5259, 0.3000, 0.6004, 0.2894, 0.4077],\n          [0.8279, 0.7619, 0.8626, 0.4019, 0.8703],\n          [1.1576, 0.7582, 1.1252, 0.3583, 0.8744]],\n\n         [[1.8753, 1.0986, 1.9939, 0.7821, 1.4981],\n          [1.6079, 0.9101, 1.8997, 0.9157, 1.5089],\n          [1.4465, 0.8693, 1.6667, 0.8347, 1.1599]],\n\n         [[1.4186, 1.1526, 1.4041, 0.5483, 1.2553],\n          [1.9908, 1.2367, 2.1596, 0.8922, 1.7607],\n          [1.0949, 0.5109, 1.1055, 0.3661, 0.5976]]],\n\n\n        [[[0.3232, 0.3395, 0.2736, 0.4738, 0.5532],\n          [0.7879, 0.4619, 0.4811, 0.7820, 0.8044],\n          [0.7571, 0.8372, 0.8546, 1.0064, 0.7694]],\n\n         [[1.0566, 1.3344, 1.3176, 1.7892, 1.6994],\n          [0.8136, 1.0792, 1.0664, 1.7680, 2.0364],\n          [0.9499, 0.8993, 0.7038, 1.2998, 1.5714]],\n\n         [[1.2052, 0.8936, 0.8995, 1.2275, 1.0797],\n          [1.1456, 1.3835, 1.4295, 2.0276, 2.0094],\n          [0.5575, 0.8357, 0.7197, 0.8475, 0.6852]]]])\n```\n:::\n:::\n\n\n::: {#b4a87d69 .cell execution_count=32}\n``` {.python .cell-code}\nres.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\ntorch.Size([2, 3, 3, 5])\n```\n:::\n:::\n\n\nHere it is. Nice and warm.\n\nIt went through a very similar route as our recruits from before and came out\njust as it should.\n\nNumpy is tinygrad's backend. So underneath, there are [C\narrays](https://www.nature.com/articles/s41586-020-2649-2).\n\nWhat about PyTorch, you say? We'll drown in ATen maybe next time.  \nBring a mace.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}