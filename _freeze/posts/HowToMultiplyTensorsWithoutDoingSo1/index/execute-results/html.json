{
  "hash": "b9e676e1d68dbfdbc944e5e8e8c36401",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How to multiply tensors wihout doing so? (1/3)\"\nauthor: \"Andre MiroÅ„czuk\"\ndate: \"2024-08-15\"\ncategories: [pytorch, tensors]\nimage: \"./tensor_transpose.png\"\njupyter: python3\n---\n\n\nLet's use these two simple 2nd-order tensors.\n\n![](./simple_tensors.png){fig-align=\"center\"}\n\nAfter multiplying them we get:\n\n::: {#7df499ef .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nt1 = torch.tensor([[1, 2], [3, 4]])\nt2 = torch.tensor([[3, 4], [1, 2]])\nt1@t2\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\ntensor([[ 5,  8],\n        [13, 20]])\n```\n:::\n:::\n\n\n![](./simple_matmul.png){fig-align=\"center\"}\n\nSimple.\n\nBut we've used a magic symbol `@`! That's a big no-no.\n\nWhat's happening there?\n\nHow about we don't do that?\n\nAs you probably know, naively looping over tensors, multiplying elements one by one,\nsumming them, and then putting them into a new tensor of the correct size is a\nbad idea.\n\nInstead, we will start by transposing the second tensor (rotating it over its\n'identity' axis).\n\n::: {#b38aefba .cell execution_count=2}\n``` {.python .cell-code}\nt2 = t2.T\nt2\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\ntensor([[3, 1],\n        [4, 2]])\n```\n:::\n:::\n\n\n![](./tensor_transpose.png){fig-align=\"center\"}\n\nWhy? We'll see in a bit.\n\nNow we want to add a dummy dimension to both tensors. We want the first\ntensor's shape to be [2, 1, 2], and the second to be [1, 2, 2].\n\nWe can simply reshape them or achieve that by adding two pairs of brackets to the\nfirst one and one pair of brackets to the second.\n\n::: {#7376c6d8 .cell execution_count=3}\n``` {.python .cell-code}\nt1 = t1.reshape((2, 1, 2)) # torch.tensor([[[1, 2]], [[3, 4]]])\nt2 = t2.reshape((1, 2, 2)) # torch.tensor([[[3, 4], [1, 2]]])\n```\n:::\n\n\n![](./tensor_reshaping.png){fig-align=\"center\"}\n\n::: {#d59f0570 .cell execution_count=4}\n``` {.python .cell-code}\nt1, t2, t1.shape, t2.shape\n```\n:::\n\n\n::: {#9a5dfcbf .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[[1, 2]],\n\n        [[3, 4]]])\ntensor([[[3, 1],\n         [4, 2]]])\ntorch.Size([2, 1, 2]) torch.Size([1, 2, 2])\n```\n:::\n:::\n\n\nLet's multiply them now (element-wise).\n\n::: {#06644ab4 .cell execution_count=6}\n``` {.python .cell-code}\nt3 = t1 * t2\n```\n:::\n\n\nWhat will come out of it exactly?\n\nMultiplication rules tell us that multiplying a [2, 1, 2] tensor by a [1, 2, 2] one\nis indeed possible and will result in a [2, 2, 2] tensor.\n\n::: {.callout-tip appearance=\"simple\"}\n## Broadcasting rules\n* Each tensor has at least one dimension.\n* When iterating over the dimension sizes, starting at the trailing dimension,\n  the dimension sizes must either be equal, one of them is 1, or one of them does\n  not exist.\n:::\n\nAs we can see, this operation doesn't brake any rules. The second dimension in\nthe first tensor and the first dimension in the second tensor will be expanded.\nImportantly, this does not make any copies of the data.\n\n::: {.callout-note appearance=\"simple\"}\nAs a side note, it's good to remember that in-place operations do not allow the in-place\ntensor to change shape.\n:::\n\nMultiplication rules in this type of scenario can be a bit tricky at first, but\nhere it is quite streightforward.\n\n![](./t1_t2_stripped.png){fig-align=\"center\"}\n\nHigh-level overview:  \nMultiply A by CD block and B by CD block. Then just put them side by side. This again uses\nbroadcasting.\n\nYou can simplify it furher:  \n\n1. Multiply A * C. Then you multiply A * D and append it to the frist vector.\n\n![](./t1_t2_first_operation.png){fig-align=\"center\"}\n\n2. Now do the same with B. B * C, B * D, append.\n\n![](./t1_t2_second_operation.png){fig-align=\"center\"}\n\n3. Since B 'is in' a different dimension than A, the resulting tensor will be\nseperate from the previous one. They will be appended together as two 2-by-2\nblocks.\n\n![](./t1_t2_third_operation.png){fig-align=\"center\"}\n\nBoth (1.) and (2.) will, sort of, take care of increasing the size of the second\ndimension of the second tensor to 2:  \n[2, 1, 2] -> [2, 2, 2]  \n(3.) will do the same thing but to the first dimension of the first tensor:  \n[1, 2, 2] -> [2, 2, 2]\n\nWe need to do one last thing, which is to sum that tensor over its last\ndimension (third one).\n\n::: {#404687f9 .cell execution_count=7}\n``` {.python .cell-code}\nt3.sum(dim=2)\n```\n:::\n\n\nThis will shrink all the vectors in the last dimension to scalars by summing\nall the numbers inside.\n\nSince the `keepdim` flag in `sum()` is set to `False` by default,\ndimensions of size 1 will be squizzed out, leaving us with a tensor of size [2,\n2].\n\n::: {#27943c6c .cell execution_count=8}\n``` {.python .cell-code}\nt3.sum(dim=2).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\ntorch.Size([2, 2])\n```\n:::\n:::\n\n\nIf we set it to `True`, the resulting tensor's shape would be [2, 2, 1].\n\n::: {#012bce22 .cell execution_count=9}\n``` {.python .cell-code}\nt3.sum(dim=2, keepdim=True).shape\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\ntorch.Size([2, 2, 1])\n```\n:::\n:::\n\n\nLet's stick with the defaults here.\n\nFinal result, after the summation and squeezing out the last dimension:\n\n::: {#425d6d34 .cell execution_count=10}\n``` {.python .cell-code}\nt3.sum(dim=2)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\ntensor([[ 5,  8],\n        [13, 20]])\n```\n:::\n:::\n\n\n![](./simple_matmul.png){fig-align=\"center\"}\n\nIt's exactly the same as with `@`!\n\nBut why?\n\nDoes it work for all shapes and is it really what happens under the hood?\n\nFind out in the [second part](../HowToMultiplyTensorsWithoutDoingSo2/index.qmd) of this blog!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}