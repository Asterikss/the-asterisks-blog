[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Asterisks Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\n\n2024-07-01\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to multiply tensors wihout doing so? (1/2)\n\n\n\n\n\n\n\n\n2024-06-28\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Andre Mirończuk",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "How to multiply matrixes wihout doing so?",
    "section": "",
    "text": "Lets use these two simple 2 by 2 matrixes.\n\nAfter multiplying them we get:\n\nimport torch\nm1 = torch.tensor([[1, 2], [3, 4]])\nm2 = torch.tensor([[3, 4], [1, 2]])\nm1@m2\n\ntensor([[ 5,  8],\n        [13, 20]])\n\n\nSimple.\nBut we’ve used a magic symbol @! That’s a big no no.\nWhat is happening there?\nHow about we dont do that.\nAs you probably know, looping over the whole matrix, mutiplying the elements one by one, summing them and then putting them in a new matrix of a correct size is a bad idea.\nInstead we will start by transposing the second matrix (rotating it over its ‘identity’ axis).\n\nm2 = m2.T\nm2\n\ntensor([[3, 1],\n        [4, 2]])\n\n\nimage\nWhy? We’ll see in a bit.\nNow we want to add a dummy dimension to both matrixes. We want the first matrix’s shape to be 2 by 1 by 2 and the second to be 1 by 2 by 2.\nWe can just reshape them or achieve that by adding two pairs of brackets to the first one and one pair of brackets to the second.\nimage\n\nm1 = m1.reshape((2, 1, 2)) # torch.tensor([[[1, 2]], [[3, 4]]])\nm2 = m2.reshape((1, 2, 2)) # torch.tensor([[[3, 4], [1, 2]]])\n\n\n\ntensor([[[1, 2]],\n\n        [[3, 4]]])\ntensor([[[3, 1],\n         [4, 2]]])\ntorch.Size([2, 1, 2]) torch.Size([1, 2, 2])\n\n\nLet’s multiply them now.\n\nm3 = m1 * m2\n\nWhat will come out of it exactly?\nMultiplication rules tell us that multiplying a [2, 1, 2] matrix by a [1, 2, 2] one is indeed possible and will result in a [2, 2, 2] matrix.\n\n\n\n\n\n\nBroadcasting rules\n\n\n\n\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\n\nAs we can see that operation doesn’t brake any rules. Second dimention in the first matrix and first dimention in the second matrix will be expanded. Importantly, this does not make any copies of the data.\n\n\n\n\n\n\nAs a side note it’s good to remember that in-place operations do not allow the in-place matrix (tensor) to change shape.\n\n\n\n\nabout\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HowToMultiplyMatrixesWithoutDoingSo1/index.html",
    "href": "posts/HowToMultiplyMatrixesWithoutDoingSo1/index.html",
    "title": "How to multiply tensors wihout doing so? (1/2)",
    "section": "",
    "text": "Lets use these two simple 2nd-order tensors.\n\nAfter multiplying them we get:\n\nimport torch\nm1 = torch.tensor([[1, 2], [3, 4]])\nm2 = torch.tensor([[3, 4], [1, 2]])\nm1@m2\n\ntensor([[ 5,  8],\n        [13, 20]])\n\n\n\nSimple.\nBut we’ve used a magic symbol @! That’s a big no no.\nWhat is happening there?\nHow about we dont do that.\nAs you probably know, looping over tensors, mutiplying elements one by one, summing them and then putting them in a new tensor of a correct size is a bad idea.\nInstead we will start by transposing the second tensor (rotating it over its ‘identity’ axis).\n\nm2 = m2.T\nm2\n\ntensor([[3, 1],\n        [4, 2]])\n\n\n\nWhy? We’ll see in a bit.\nNow we want to add a dummy dimension to both tensors. We want the first tensors’s shape to be [2, 1, 2] and the second to be [1, 2, 2].\nWe can just reshape them or achieve that by adding two pairs of brackets to the first one and one pair of brackets to the second.\n\nm1 = m1.reshape((2, 1, 2)) # torch.tensor([[[1, 2]], [[3, 4]]])\nm2 = m2.reshape((1, 2, 2)) # torch.tensor([[[3, 4], [1, 2]]])\n\n\n\nm1, m2, m1.shape, m2.shape\n\n\n\ntensor([[[1, 2]],\n\n        [[3, 4]]])\ntensor([[[3, 1],\n         [4, 2]]])\ntorch.Size([2, 1, 2]) torch.Size([1, 2, 2])\n\n\nLet’s multiply them now (element-wise).\n\nm3 = m1 * m2\n\nWhat will come out of it exactly?\nMultiplication rules tell us that multiplying a [2, 1, 2] tensor by a [1, 2, 2] one is indeed possible and will result in a [2, 2, 2] tensor.\n\n\n\n\n\n\nBroadcasting rules\n\n\n\n\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\n\nAs we can see that this operation doesn’t brake any rules. Second dimention in the first tensor and first dimention in the second tensor will be expanded. Importantly, this does not make any copies of the data.\n\n\n\n\n\n\nAs a side note it’s good to remember that in-place operations do not allow the in-place tensor to change shape.\n\n\n\nMultiplication rules in this type of scenario can be a bit tricky at first, but here it is quite streight forward.\n\nHigh overview:\nMultiply A by m2 and B by m2. Then just put them side by side. This again uses broadcasting.\nYou can simplify it furher:\n\nMultiply A * C. Then you multiply A * D and append it to the frist vector  \n\n\n\nNow do the same with B. B * C, B * D, append.\n\n\n\nSince B ‘is in’ a different dimension than A, the resulting tensor will be seperate from the previous one. They will be appended together as two 2 by 2 blocks.\n\n\nBoth (1.) and (2.) will, sort of, take care of increasing the size of the second dimension of the second tensor to 2:\n[2, 1, 2] -&gt; [2, 2, 2]\n(3.) will do the same but to the first demention of the first tensor:\n[1, 2, 2] -&gt; [2, 2, 2]\nWe need to do one last thing, that is to sum that tensor over its second dimension (last one).\n\nm3.sum(dim=2)\n\nThis will shrink all the vectors in the last dimension to scalars by summing all the numbers inside.\nThen, since the keepdim flag in sum() is set to False by default, dimensions of size 1 will be squizzed out, leaving us with a tensor of size [2, 2].\n\n\n\n\n\nFinal result, after the last dimension is squeezed out:\n\n\ntensor([[ 5,  8],\n        [13, 20]])\n\n\n\nIt’s exactly the same as with @!\nBut why?\nDoes it always work and is it really what happens under the hood?\nFind out in the second part of this blog!"
  },
  {
    "objectID": "posts/HowToMultiplyTensorsWithoutDoingSo1/index.html",
    "href": "posts/HowToMultiplyTensorsWithoutDoingSo1/index.html",
    "title": "How to multiply tensors wihout doing so? (1/2)",
    "section": "",
    "text": "Lets use these two simple 2nd-order tensors.\n\nAfter multiplying them we get:\n\nimport torch\nm1 = torch.tensor([[1, 2], [3, 4]])\nm2 = torch.tensor([[3, 4], [1, 2]])\nm1@m2\n\ntensor([[ 5,  8],\n        [13, 20]])\n\n\n\nSimple.\nBut we’ve used a magic symbol @! That’s a big no no.\nWhat is happening there?\nHow about we dont do that.\nAs you probably know, looping over tensors, mutiplying elements one by one, summing them and then putting them in a new tensor of a correct size is a bad idea.\nInstead we will start by transposing the second tensor (rotating it over its ‘identity’ axis).\n\nm2 = m2.T\nm2\n\ntensor([[3, 1],\n        [4, 2]])\n\n\n\nWhy? We’ll see in a bit.\nNow we want to add a dummy dimension to both tensors. We want the first tensors’s shape to be [2, 1, 2] and the second to be [1, 2, 2].\nWe can just reshape them or achieve that by adding two pairs of brackets to the first one and one pair of brackets to the second.\n\nm1 = m1.reshape((2, 1, 2)) # torch.tensor([[[1, 2]], [[3, 4]]])\nm2 = m2.reshape((1, 2, 2)) # torch.tensor([[[3, 4], [1, 2]]])\n\n\n\nm1, m2, m1.shape, m2.shape\n\n\n\ntensor([[[1, 2]],\n\n        [[3, 4]]])\ntensor([[[3, 1],\n         [4, 2]]])\ntorch.Size([2, 1, 2]) torch.Size([1, 2, 2])\n\n\nLet’s multiply them now (element-wise).\n\nm3 = m1 * m2\n\nWhat will come out of it exactly?\nMultiplication rules tell us that multiplying a [2, 1, 2] tensor by a [1, 2, 2] one is indeed possible and will result in a [2, 2, 2] tensor.\n\n\n\n\n\n\nBroadcasting rules\n\n\n\n\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\n\nAs we can see that this operation doesn’t brake any rules. Second dimention in the first tensor and first dimention in the second tensor will be expanded. Importantly, this does not make any copies of the data.\n\n\n\n\n\n\nAs a side note it’s good to remember that in-place operations do not allow the in-place tensor to change shape.\n\n\n\nMultiplication rules in this type of scenario can be a bit tricky at first, but here it is quite streight forward.\n\nHigh overview:\nMultiply A by m2 and B by m2. Then just put them side by side. This again uses broadcasting.\nYou can simplify it furher:\n\nMultiply A * C. Then you multiply A * D and append it to the frist vector  \n\n\n\nNow do the same with B. B * C, B * D, append.\n\n\n\nSince B ‘is in’ a different dimension than A, the resulting tensor will be seperate from the previous one. They will be appended together as two 2 by 2 blocks.\n\n\nBoth (1.) and (2.) will, sort of, take care of increasing the size of the second dimension of the second tensor to 2:\n[2, 1, 2] -&gt; [2, 2, 2]\n(3.) will do the same but to the first demention of the first tensor:\n[1, 2, 2] -&gt; [2, 2, 2]\nWe need to do one last thing, that is to sum that tensor over its second dimension (last one).\n\nm3.sum(dim=2)\n\nThis will shrink all the vectors in the last dimension to scalars by summing all the numbers inside.\nThen, since the keepdim flag in sum() is set to False by default, dimensions of size 1 will be squizzed out, leaving us with a tensor of size [2, 2].\n\n\n\n\n\nFinal result, after the last dimension is squeezed out:\n\n\ntensor([[ 5,  8],\n        [13, 20]])\n\n\n\nIt’s exactly the same as with @!\nBut why?\nDoes it always work and is it really what happens under the hood?\nFind out in the second part of this blog!"
  },
  {
    "objectID": "posts/HowToMultiplyTensorsWithoutDoingSo2/index.html",
    "href": "posts/HowToMultiplyTensorsWithoutDoingSo2/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]